# Docker Compose for local development and testing
# Usage: docker compose up --build
#
# For LLM-powered bots (optional):
#   docker compose --profile llm up --build
#
# The Ollama service will auto-scale: start when a bot is added, stop after 5 min idle.

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DEBUG=true
      - CORS_ORIGINS=http://localhost,http://localhost:80,http://localhost:5173
      - REDIS_URL=redis://redis:6379/0
      - DOCKER_COMPOSE_MODE=true
      - OLLAMA_URL=http://ollama:11434
    volumes:
      # Mount Docker socket for container management (LLM auto-scaling)
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/api/health')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - VITE_API_BASE_URL=/api
        - VITE_WS_HOST=
    ports:
      - "80:80"
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  # Ollama LLM service for intelligent bot responses
  # This service is managed by the backend's LLM manager for auto-scaling
  ollama:
    image: ollama/ollama:latest
    profiles:
      - llm
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  redis_data:
  ollama_data:
    # Persists downloaded models between container restarts
